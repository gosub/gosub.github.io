<!DOCTYPE html>
<html lang="en-us">
<head>
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> All the papers cited in Andrej Karpathy&#39;s YouTube videos | Giampaolo Guiducci</title>
  <link rel = 'canonical' href = 'https://giampaolo.guiducci.it/posts/2026-02-09-karpathy-video-papers/'>
  <meta name="description" content="An empty set, by construction.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:url" content="https://giampaolo.guiducci.it/posts/2026-02-09-karpathy-video-papers/">
  <meta property="og:site_name" content="Giampaolo Guiducci">
  <meta property="og:title" content="All the papers cited in Andrej Karpathy&#39;s YouTube videos">
  <meta property="og:description" content="I’ve been following Andrej Karpathy’s YouTube channel for a long time, and he made me want to dive deep into AI, Neural Networks and LLMs. His videos are interesting, precise and engaging, and they pushed me to read primary sources, i.e. papers and articles. I’ve collected here all the papers that were mentioned in the various videos, as a personal reminder to read them all.
Since I’ve already seen all the videos and didn’t want to watch them all again just to find the papers, I’ve downloaded the subtitle with the command below, and searched for all the occurrences of the term “paper”, then looked at the corresponding timestamps.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2026-02-09T20:51:32+01:00">
    <meta property="article:modified_time" content="2026-02-09T20:51:32+01:00">
    <meta property="article:tag" content="ai">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="All the papers cited in Andrej Karpathy&#39;s YouTube videos">
  <meta name="twitter:description" content="I’ve been following Andrej Karpathy’s YouTube channel for a long time, and he made me want to dive deep into AI, Neural Networks and LLMs. His videos are interesting, precise and engaging, and they pushed me to read primary sources, i.e. papers and articles. I’ve collected here all the papers that were mentioned in the various videos, as a personal reminder to read them all.
Since I’ve already seen all the videos and didn’t want to watch them all again just to find the papers, I’ve downloaded the subtitle with the command below, and searched for all the occurrences of the term “paper”, then looked at the corresponding timestamps.">

  
  
  
  <link rel="stylesheet" href="https://giampaolo.guiducci.it/css/styles.94f653e9e151e28067a7c5dbbc4600cbd5a3c721e79faaf971e523c40f3b249b8e4f20bb57810dfffa8d559ca5c140fd56eb4cd9c0853113ad08e66afdb08bdd.css" integrity="sha512-lPZT6eFR4oBnp8XbvEYAy9WjxyHnn6r5ceUjxA87JJuOTyC7V4EN//qNVZylwUD9VutM2cCFMROtCOZq/bCL3Q=="> 

  
  
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="https://giampaolo.guiducci.it/images/favicon.ico" />

  
  
</head>

<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

  <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;" aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
        <li><a href="/">home</a></li>
         
        <li><a href="/posts">posts</a></li>
         
        <li><a href="/%CE%BClog">μlog</a></li>
         
        <li><a href="/tags">tags</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li>
          <a class="icon" href=" https://giampaolo.guiducci.it/posts/2026-01-05-many-faces-of-xor/" aria-label="Previous">
            <i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i>
          </a>
        </li>
        
        
        <li>
          <a class="icon" href="https://giampaolo.guiducci.it/posts/2026-02-16-mixpost-2026w08/" aria-label="Next">
            <i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i>
          </a>
        </li>
        
        <li>
          <a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
            <i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i>
          </a>
        </li>
        <li>
          <a class="icon" href="#" aria-label="Share">
            <i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i>
          </a>
        </li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f" aria-label="Facebook">
      <i class="fab fa-facebook " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&text=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="Twitter">
      <i class="fab fa-twitter " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&title=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="Linkedin">
      <i class="fab fa-linkedin " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&is_video=false&description=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="Pinterest">
      <i class="fab fa-pinterest " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos&body=Check out this article: https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f" aria-label="Email">
      <i class="fas fa-envelope " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&title=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="Pocket">
      <i class="fab fa-get-pocket " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&title=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="reddit">
      <i class="fab fa-reddit " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&name=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos&description=%3cp%3eI%26rsquo%3bve%20been%20following%20%3ca%20href%3d%22https%3a%2f%2fwww.youtube.com%2f%40AndrejKarpathy%2fvideos%22%3eAndrej%20Karpathy%26rsquo%3bs%20YouTube%20channel%3c%2fa%3e%20for%20a%20long%20time%2c%20and%20he%0amade%20me%20want%20to%20dive%20deep%20into%20AI%2c%20Neural%20Networks%20and%20LLMs.%20His%20videos%20are%0ainteresting%2c%20precise%20and%20engaging%2c%20and%20they%20pushed%20me%20to%20read%20primary%20sources%2c%0ai.e.%20papers%20and%20articles.%20I%26rsquo%3bve%20collected%20here%20all%20the%20papers%20that%20were%20mentioned%0ain%20the%20various%20videos%2c%20as%20a%20personal%20reminder%20to%20read%20them%20all.%3c%2fp%3e%0a%3cp%3eSince%20I%26rsquo%3bve%20already%20seen%20all%20the%20videos%20and%20didn%26rsquo%3bt%20want%20to%20watch%20them%20all%20again%0ajust%20to%20find%20the%20papers%2c%20I%26rsquo%3bve%20downloaded%20the%20subtitle%20with%20the%20command%20below%2c%0aand%20searched%20for%20all%20the%20occurrences%20of%20the%20term%20%26ldquo%3bpaper%26rdquo%3b%2c%20then%20looked%20at%20the%0acorresponding%20timestamps.%3c%2fp%3e" aria-label="Tumblr">
      <i class="fab fa-tumblr " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&t=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="Hacker News">
      <i class="fab fa-hacker-news " aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>
    
    <div id="toc">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#the-spelled-out-intro-to-neural-networks-and-backpropagation-building-micrograd">The spelled-out intro to neural networks and backpropagation: building micrograd</a></li>
    <li><a href="#the-spelled-out-intro-to-language-modeling-building-makemore">The spelled-out intro to language modeling: building makemore</a></li>
    <li><a href="#building-makemore-part-2-mlp">Building makemore Part 2: MLP</a>
      <ul>
        <li><a href="#bengio-et-al-2003-a-neural-probabilistic-language-model">bengio et al 2003 - A Neural Probabilistic Language Model</a></li>
      </ul>
    </li>
    <li><a href="#building-makemore-part-3-activations-and-gradients-batchnorm">Building makemore Part 3: Activations &amp; Gradients, BatchNorm</a>
      <ul>
        <li><a href="#delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
        <li><a href="#batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
      </ul>
    </li>
    <li><a href="#building-makemore-part-4-becoming-a-backprop-ninja">Building makemore Part 4: Becoming a Backprop Ninja</a>
      <ul>
        <li><a href="#reducing-the-dimensionality-of-data-with-neural-networks">Reducing the Dimensionality of Data with Neural Networks</a></li>
        <li><a href="#deep-fragment-embeddings-for-bidirectional-image-sentence-mapping">Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</a></li>
        <li><a href="#batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
        <li><a href="#bessel-s-correction">Bessel&rsquo;s Correction</a></li>
      </ul>
    </li>
    <li><a href="#building-makemore-part-5-building-a-wavenet">Building makemore Part 5: Building a WaveNet</a>
      <ul>
        <li><a href="#wavenet-a-generative-model-for-raw-audio">WaveNet: A Generative Model for Raw Audio</a></li>
        <li><a href="#bengio-et-al-2003-a-neural-probabilistic-language-model">bengio et al 2003 - A Neural Probabilistic Language Model</a></li>
      </ul>
    </li>
    <li><a href="#let-s-build-gpt-from-scratch-in-code-spelled-out-dot">Let&rsquo;s build GPT: from scratch, in code, spelled out.</a>
      <ul>
        <li><a href="#attention-is-all-you-need">Attention is All You Need</a></li>
        <li><a href="#layer-normalization">Layer Normalization</a></li>
        <li><a href="#dropout-a-simple-way-to-prevent-neural-networks-from-overfitting">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
        <li><a href="#language-models-are-few-shot-learners">Language Models are Few-Shot Learners</a></li>
        <li><a href="#introducing-chatgpt">Introducing ChatGPT</a></li>
      </ul>
    </li>
    <li><a href="#1hr-talk-intro-to-large-language-models">[1hr Talk] Intro to Large Language Models</a>
      <ul>
        <li><a href="#llama-2-open-foundation-and-fine-tuned-chat-models">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li>
        <li><a href="#training-language-models-to-follow-instructions-with-human-feedback">Training language models to follow instructions with human feedback</a></li>
        <li><a href="#tree-of-thoughts-deliberate-problem-solving-with-large-language-models-yao-et-al-dot-2023">Tree of Thoughts: Deliberate Problem Solving with Large Language Models, Yao et al. 2023</a></li>
        <li><a href="#training-compute-optimal-large-language-models">Training Compute-Optimal Large Language Models</a></li>
        <li><a href="#sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4-bubuck-et-al-dot-2023">Sparks of Artificial General Intelligence: Early experiments with GPT-4, Bubuck et al. 2023</a></li>
        <li><a href="#mastering-the-game-of-go-with-deep-neural-networks-and-tree-search">Mastering the game of Go with deep neural networks and tree search</a></li>
        <li><a href="#jailbroken-how-does-llm-safety-training-fail">Jailbroken: How Does LLM Safety Training Fail?</a></li>
        <li><a href="#universal-and-transferable-adversarial-attacks-on-aligned-language-models">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
        <li><a href="#visual-adversarial-examples-jailbreak-aligned-large-language-models">Visual Adversarial Examples Jailbreak Aligned Large Language Models</a></li>
        <li><a href="#not-what-you-ve-signed-up-for-compromising-real-world-llm-integrated-applications-with-indirect-prompt-injection">Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</a></li>
        <li><a href="#hacking-google-bard-from-prompt-injection-to-data-exfiltration">Hacking Google Bard - From Prompt Injection to Data Exfiltration</a></li>
        <li><a href="#poisoning-language-models-during-instruction-tuning">Poisoning Language Models During Instruction Tuning</a></li>
        <li><a href="#poisoning-web-scale-training-datasets-is-practical">Poisoning Web-Scale Training Datasets is Practical</a></li>
        <li><a href="#owasp-top-10-for-llm-applications">OWASP Top 10 for LLM Applications</a></li>
      </ul>
    </li>
    <li><a href="#let-s-build-the-gpt-tokenizer">Let&rsquo;s build the GPT Tokenizer</a>
      <ul>
        <li><a href="#language-models-are-unsupervised-multitask-learners">Language Models are Unsupervised Multitask Learners</a></li>
        <li><a href="#llama-2-open-foundation-and-fine-tuned-chat-models">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li>
        <li><a href="#a-programmer-s-introduction-to-unicode">A Programmer’s Introduction to Unicode</a></li>
        <li><a href="#megabyte-predicting-million-byte-sequences-with-multiscale-transformers">MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers</a></li>
        <li><a href="#efficient-training-of-language-models-to-fill-in-the-middle">Efficient Training of Language Models to Fill in the Middle</a></li>
        <li><a href="#learning-to-compress-prompts-with-gist-tokens">Learning to Compress Prompts with Gist Tokens</a></li>
        <li><a href="#taming-transformers-for-high-resolution-image-synthesis">Taming Transformers for High-Resolution Image Synthesis</a></li>
        <li><a href="#video-generation-models-as-world-simulators">Video generation models as world simulators</a></li>
        <li><a href="#integer-tokenization-is-insane">Integer tokenization is insane</a></li>
        <li><a href="#solidgoldmagikarp--plus-prompt-generation">SolidGoldMagikarp (plus, prompt generation)</a></li>
      </ul>
    </li>
    <li><a href="#let-s-reproduce-gpt-2--124m">Let&rsquo;s reproduce GPT-2 (124M)</a>
      <ul>
        <li><a href="#language-models-are-unsupervised-multitask-learners">Language Models are Unsupervised Multitask Learners</a></li>
        <li><a href="#language-models-are-few-shot-learners">Language Models are Few-Shot Learners</a></li>
        <li><a href="#attention-is-all-you-need">Attention is All You Need</a></li>
        <li><a href="#gaussian-error-linear-units--gelus">Gaussian Error Linear Units (GELUs)</a></li>
        <li><a href="#using-the-output-embedding-to-improve-language-models">Using the Output Embedding to Improve Language Models</a></li>
        <li><a href="#nvidia-a100-tensor-core-gpu-architecture">NVIDIA A100 Tensor Core GPU Architecture</a></li>
        <li><a href="#flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li>
        <li><a href="#flashattention-2-faster-attention-with-better-parallelism-and-work-partitioning">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></li>
        <li><a href="#online-normalizer-calculation-for-softmax">Online normalizer calculation for softmax</a></li>
        <li><a href="#hellaswag-can-a-machine-really-finish-your-sentence">HellaSwag: Can a Machine Really Finish Your Sentence?</a></li>
      </ul>
    </li>
    <li><a href="#deep-dive-into-llms-like-chatgpt">Deep Dive into LLMs like ChatGPT</a>
      <ul>
        <li><a href="#language-models-are-unsupervised-multitask-learners">Language Models are Unsupervised Multitask Learners</a></li>
        <li><a href="#the-llama-3-herd-of-models">The Llama 3 Herd of Models</a></li>
        <li><a href="#training-language-models-to-follow-instructions-with-human-feedback">Training language models to follow instructions with human feedback</a></li>
        <li><a href="#deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></li>
        <li><a href="#mastering-the-game-of-go-without-human-knowledge">Mastering the Game of Go without Human Knowledge</a></li>
        <li><a href="#fine-tuning-language-models-from-human-preferences">Fine-Tuning Language Models from Human Preferences</a></li>
      </ul>
    </li>
    <li><a href="#how-i-use-llms">How I use LLMs</a>
      <ul>
        <li><a href="#deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
    
  </span>
</div>


  <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
    <header>
      <h1 class="posttitle" itemprop="name headline">
        All the papers cited in Andrej Karpathy&#39;s YouTube videos
      </h1>
      <div class="meta">
        
        <div class="postdate">
          
          <time datetime="2026-02-09 20:51:32 &#43;0100 CET" itemprop="datePublished">2026-02-09</time>
          
        </div>
        
        
        
        
        <div class="article-tag">
            <i class="fas fa-tag"></i>
            
            
            <a class="tag-link" href="/tags/ai" rel="tag">ai</a>
            
        </div>
        
      </div>
    </header>

  
    
    <div class="content" itemprop="articleBody">
      <p>I&rsquo;ve been following <a href="https://www.youtube.com/@AndrejKarpathy/videos">Andrej Karpathy&rsquo;s YouTube channel</a> for a long time, and he
made me want to dive deep into AI, Neural Networks and LLMs. His videos are
interesting, precise and engaging, and they pushed me to read primary sources,
i.e. papers and articles. I&rsquo;ve collected here all the papers that were mentioned
in the various videos, as a personal reminder to read them all.</p>
<p>Since I&rsquo;ve already seen all the videos and didn&rsquo;t want to watch them all again
just to find the papers, I&rsquo;ve downloaded the subtitle with the command below,
and searched for all the occurrences of the term &ldquo;paper&rdquo;, then looked at the
corresponding timestamps.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>yt-dlp --skip-download --write-subs --write-auto-subs --sub-lang en --sub-format ttml --convert-subs srt --output <span style="color:#e6db74">&#34;transcript_%(id)s_%(title)s.%(ext)s&#34;</span> <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>
</span></span></code></pre></div><p>Here is the list of videos, papers and pdf files:</p>
<h2 id="the-spelled-out-intro-to-neural-networks-and-backpropagation-building-micrograd">The spelled-out intro to neural networks and backpropagation: building micrograd</h2>
<p><a href="https://www.youtube.com/watch?v=VMj-3S1tku0">https://www.youtube.com/watch?v=VMj-3S1tku0</a>
No papers mentioned.</p>
<h2 id="the-spelled-out-intro-to-language-modeling-building-makemore">The spelled-out intro to language modeling: building makemore</h2>
<p><a href="https://www.youtube.com/watch?v=PaCmpygFfXo">https://www.youtube.com/watch?v=PaCmpygFfXo</a>
No papers mentioned.</p>
<h2 id="building-makemore-part-2-mlp">Building makemore Part 2: MLP</h2>
<p><a href="https://www.youtube.com/watch?v=TCH_1BHY58I">https://www.youtube.com/watch?v=TCH_1BHY58I</a></p>
<h3 id="bengio-et-al-2003-a-neural-probabilistic-language-model">bengio et al 2003 - A Neural Probabilistic Language Model</h3>
<p><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a></p>
<h2 id="building-makemore-part-3-activations-and-gradients-batchnorm">Building makemore Part 3: Activations &amp; Gradients, BatchNorm</h2>
<p><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc">https://www.youtube.com/watch?v=P6sfmUTpUmc</a></p>
<h3 id="delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</h3>
<p><a href="https://arxiv.org/abs/1502.01852">https://arxiv.org/abs/1502.01852</a></p>
<h3 id="batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</h3>
<p><a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a></p>
<h2 id="building-makemore-part-4-becoming-a-backprop-ninja">Building makemore Part 4: Becoming a Backprop Ninja</h2>
<p><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI">https://www.youtube.com/watch?v=q8SA3rM6ckI</a></p>
<h3 id="reducing-the-dimensionality-of-data-with-neural-networks">Reducing the Dimensionality of Data with Neural Networks</h3>
<p><a href="https://www.cs.toronto.edu/~hinton/absps/science.pdf">https://www.cs.toronto.edu/~hinton/absps/science.pdf</a></p>
<h3 id="deep-fragment-embeddings-for-bidirectional-image-sentence-mapping">Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</h3>
<p><a href="https://arxiv.org/abs/1406.5679">https://arxiv.org/abs/1406.5679</a></p>
<h3 id="batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</h3>
<p><a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a></p>
<h3 id="bessel-s-correction">Bessel&rsquo;s Correction</h3>
<p><a href="https://mathcenter.oxford.emory.edu/site/math117/besselCorrection/">https://mathcenter.oxford.emory.edu/site/math117/besselCorrection/</a></p>
<h2 id="building-makemore-part-5-building-a-wavenet">Building makemore Part 5: Building a WaveNet</h2>
<p><a href="https://www.youtube.com/watch?v=t3YJ5hKiMQ0">https://www.youtube.com/watch?v=t3YJ5hKiMQ0</a></p>
<h3 id="wavenet-a-generative-model-for-raw-audio">WaveNet: A Generative Model for Raw Audio</h3>
<p><a href="https://arxiv.org/abs/1609.03499">https://arxiv.org/abs/1609.03499</a></p>
<h3 id="bengio-et-al-2003-a-neural-probabilistic-language-model">bengio et al 2003 - A Neural Probabilistic Language Model</h3>
<p><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a></p>
<h2 id="let-s-build-gpt-from-scratch-in-code-spelled-out-dot">Let&rsquo;s build GPT: from scratch, in code, spelled out.</h2>
<p><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">https://www.youtube.com/watch?v=kCc8FmEb1nY</a></p>
<h3 id="attention-is-all-you-need">Attention is All You Need</h3>
<p><a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>
<h3 id="layer-normalization">Layer Normalization</h3>
<p><a href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a></p>
<h3 id="dropout-a-simple-way-to-prevent-neural-networks-from-overfitting">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</h3>
<p><a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a></p>
<h3 id="language-models-are-few-shot-learners">Language Models are Few-Shot Learners</h3>
<p><a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></p>
<h3 id="introducing-chatgpt">Introducing ChatGPT</h3>
<p><a href="https://openai.com/index/chatgpt/">https://openai.com/index/chatgpt/</a></p>
<h2 id="1hr-talk-intro-to-large-language-models">[1hr Talk] Intro to Large Language Models</h2>
<p><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">https://www.youtube.com/watch?v=zjkBMFhNj_g</a></p>
<h3 id="llama-2-open-foundation-and-fine-tuned-chat-models">Llama 2: Open Foundation and Fine-Tuned Chat Models</h3>
<p><a href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a></p>
<h3 id="training-language-models-to-follow-instructions-with-human-feedback">Training language models to follow instructions with human feedback</h3>
<p><a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a></p>
<h3 id="tree-of-thoughts-deliberate-problem-solving-with-large-language-models-yao-et-al-dot-2023">Tree of Thoughts: Deliberate Problem Solving with Large Language Models, Yao et al. 2023</h3>
<p><a href="https://arxiv.org/abs/2305.10601">https://arxiv.org/abs/2305.10601</a></p>
<h3 id="training-compute-optimal-large-language-models">Training Compute-Optimal Large Language Models</h3>
<p><a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a></p>
<h3 id="sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4-bubuck-et-al-dot-2023">Sparks of Artificial General Intelligence: Early experiments with GPT-4, Bubuck et al. 2023</h3>
<p><a href="https://arxiv.org/abs/2303.12712">https://arxiv.org/abs/2303.12712</a></p>
<h3 id="mastering-the-game-of-go-with-deep-neural-networks-and-tree-search">Mastering the game of Go with deep neural networks and tree search</h3>
<p><a href="https://www.researchgate.net/publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search">https://www.researchgate.net/publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search</a></p>
<h3 id="jailbroken-how-does-llm-safety-training-fail">Jailbroken: How Does LLM Safety Training Fail?</h3>
<p><a href="https://arxiv.org/abs/2307.02483">https://arxiv.org/abs/2307.02483</a></p>
<h3 id="universal-and-transferable-adversarial-attacks-on-aligned-language-models">Universal and Transferable Adversarial Attacks on Aligned Language Models</h3>
<p><a href="https://arxiv.org/abs/2307.15043">https://arxiv.org/abs/2307.15043</a></p>
<h3 id="visual-adversarial-examples-jailbreak-aligned-large-language-models">Visual Adversarial Examples Jailbreak Aligned Large Language Models</h3>
<p><a href="https://arxiv.org/abs/2306.13213">https://arxiv.org/abs/2306.13213</a></p>
<h3 id="not-what-you-ve-signed-up-for-compromising-real-world-llm-integrated-applications-with-indirect-prompt-injection">Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</h3>
<p><a href="https://arxiv.org/abs/2302.12173">https://arxiv.org/abs/2302.12173</a></p>
<h3 id="hacking-google-bard-from-prompt-injection-to-data-exfiltration">Hacking Google Bard - From Prompt Injection to Data Exfiltration</h3>
<p><a href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/">https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/</a></p>
<h3 id="poisoning-language-models-during-instruction-tuning">Poisoning Language Models During Instruction Tuning</h3>
<p><a href="https://arxiv.org/abs/2305.00944">https://arxiv.org/abs/2305.00944</a></p>
<h3 id="poisoning-web-scale-training-datasets-is-practical">Poisoning Web-Scale Training Datasets is Practical</h3>
<p><a href="https://arxiv.org/abs/2302.10149">https://arxiv.org/abs/2302.10149</a></p>
<h3 id="owasp-top-10-for-llm-applications">OWASP Top 10 for LLM Applications</h3>
<p><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/">https://owasp.org/www-project-top-10-for-large-language-model-applications/</a></p>
<h2 id="let-s-build-the-gpt-tokenizer">Let&rsquo;s build the GPT Tokenizer</h2>
<p><a href="https://www.youtube.com/watch?v=zduSFxRajkE">https://www.youtube.com/watch?v=zduSFxRajkE</a></p>
<h3 id="language-models-are-unsupervised-multitask-learners">Language Models are Unsupervised Multitask Learners</h3>
<p><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></p>
<h3 id="llama-2-open-foundation-and-fine-tuned-chat-models">Llama 2: Open Foundation and Fine-Tuned Chat Models</h3>
<p><a href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a></p>
<h3 id="a-programmer-s-introduction-to-unicode">A Programmer’s Introduction to Unicode</h3>
<p><a href="https://www.reedbeta.com/blog/programmers-intro-to-unicode/">https://www.reedbeta.com/blog/programmers-intro-to-unicode/</a></p>
<h3 id="megabyte-predicting-million-byte-sequences-with-multiscale-transformers">MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers</h3>
<p><a href="https://arxiv.org/abs/2305.07185">https://arxiv.org/abs/2305.07185</a></p>
<h3 id="efficient-training-of-language-models-to-fill-in-the-middle">Efficient Training of Language Models to Fill in the Middle</h3>
<p><a href="https://arxiv.org/abs/2207.14255">https://arxiv.org/abs/2207.14255</a></p>
<h3 id="learning-to-compress-prompts-with-gist-tokens">Learning to Compress Prompts with Gist Tokens</h3>
<p><a href="https://arxiv.org/abs/2304.08467">https://arxiv.org/abs/2304.08467</a></p>
<h3 id="taming-transformers-for-high-resolution-image-synthesis">Taming Transformers for High-Resolution Image Synthesis</h3>
<p><a href="https://arxiv.org/abs/2012.09841">https://arxiv.org/abs/2012.09841</a>
<a href="https://compvis.github.io/taming-transformers/">https://compvis.github.io/taming-transformers/</a></p>
<h3 id="video-generation-models-as-world-simulators">Video generation models as world simulators</h3>
<p><a href="https://openai.com/index/video-generation-models-as-world-simulators/">https://openai.com/index/video-generation-models-as-world-simulators/</a></p>
<h3 id="integer-tokenization-is-insane">Integer tokenization is insane</h3>
<p><a href="https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/">https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/</a></p>
<h3 id="solidgoldmagikarp--plus-prompt-generation">SolidGoldMagikarp (plus, prompt generation)</h3>
<p><a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation</a></p>
<h2 id="let-s-reproduce-gpt-2--124m">Let&rsquo;s reproduce GPT-2 (124M)</h2>
<p><a href="https://www.youtube.com/watch?v=l8pRSuU81PU">https://www.youtube.com/watch?v=l8pRSuU81PU</a></p>
<h3 id="language-models-are-unsupervised-multitask-learners">Language Models are Unsupervised Multitask Learners</h3>
<p><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></p>
<h3 id="language-models-are-few-shot-learners">Language Models are Few-Shot Learners</h3>
<p><a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></p>
<h3 id="attention-is-all-you-need">Attention is All You Need</h3>
<p><a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>
<h3 id="gaussian-error-linear-units--gelus">Gaussian Error Linear Units (GELUs)</h3>
<p><a href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a></p>
<h3 id="using-the-output-embedding-to-improve-language-models">Using the Output Embedding to Improve Language Models</h3>
<p><a href="https://arxiv.org/abs/1608.05859">https://arxiv.org/abs/1608.05859</a></p>
<h3 id="nvidia-a100-tensor-core-gpu-architecture">NVIDIA A100 Tensor Core GPU Architecture</h3>
<p><a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf">https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf</a></p>
<h3 id="flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</h3>
<p><a href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a></p>
<h3 id="flashattention-2-faster-attention-with-better-parallelism-and-work-partitioning">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</h3>
<p><a href="https://arxiv.org/abs/2307.08691">https://arxiv.org/abs/2307.08691</a></p>
<h3 id="online-normalizer-calculation-for-softmax">Online normalizer calculation for softmax</h3>
<p><a href="https://arxiv.org/abs/1805.02867">https://arxiv.org/abs/1805.02867</a></p>
<h3 id="hellaswag-can-a-machine-really-finish-your-sentence">HellaSwag: Can a Machine Really Finish Your Sentence?</h3>
<p><a href="https://arxiv.org/abs/1905.07830">https://arxiv.org/abs/1905.07830</a></p>
<h2 id="deep-dive-into-llms-like-chatgpt">Deep Dive into LLMs like ChatGPT</h2>
<p><a href="https://www.youtube.com/watch?v=7xTGNNLPyMI">https://www.youtube.com/watch?v=7xTGNNLPyMI</a></p>
<h3 id="language-models-are-unsupervised-multitask-learners">Language Models are Unsupervised Multitask Learners</h3>
<p><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></p>
<h3 id="the-llama-3-herd-of-models">The Llama 3 Herd of Models</h3>
<p><a href="https://arxiv.org/abs/2407.21783">https://arxiv.org/abs/2407.21783</a></p>
<h3 id="training-language-models-to-follow-instructions-with-human-feedback">Training language models to follow instructions with human feedback</h3>
<p><a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a></p>
<h3 id="deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h3>
<p><a href="https://arxiv.org/abs/2501.12948">https://arxiv.org/abs/2501.12948</a></p>
<h3 id="mastering-the-game-of-go-without-human-knowledge">Mastering the Game of Go without Human Knowledge</h3>
<p><a href="https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf">https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf</a></p>
<h3 id="fine-tuning-language-models-from-human-preferences">Fine-Tuning Language Models from Human Preferences</h3>
<p><a href="https://arxiv.org/abs/1909.08593">https://arxiv.org/abs/1909.08593</a></p>
<h2 id="how-i-use-llms">How I use LLMs</h2>
<p><a href="https://www.youtube.com/watch?v=EWvNQjAaOHw">https://www.youtube.com/watch?v=EWvNQjAaOHw</a></p>
<h3 id="deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h3>
<p><a href="https://arxiv.org/abs/2501.12948">https://arxiv.org/abs/2501.12948</a></p>

    </div>
  </article>

  
  






  <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">home</a></li>
         
          <li><a href="/posts">posts</a></li>
         
          <li><a href="/%CE%BClog">μlog</a></li>
         
          <li><a href="/tags">tags</a></li>
        
      </ul>
    </div>

    
    <div id="toc-footer" style="display: none">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#the-spelled-out-intro-to-neural-networks-and-backpropagation-building-micrograd">The spelled-out intro to neural networks and backpropagation: building micrograd</a></li>
    <li><a href="#the-spelled-out-intro-to-language-modeling-building-makemore">The spelled-out intro to language modeling: building makemore</a></li>
    <li><a href="#building-makemore-part-2-mlp">Building makemore Part 2: MLP</a>
      <ul>
        <li><a href="#bengio-et-al-2003-a-neural-probabilistic-language-model">bengio et al 2003 - A Neural Probabilistic Language Model</a></li>
      </ul>
    </li>
    <li><a href="#building-makemore-part-3-activations-and-gradients-batchnorm">Building makemore Part 3: Activations &amp; Gradients, BatchNorm</a>
      <ul>
        <li><a href="#delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
        <li><a href="#batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
      </ul>
    </li>
    <li><a href="#building-makemore-part-4-becoming-a-backprop-ninja">Building makemore Part 4: Becoming a Backprop Ninja</a>
      <ul>
        <li><a href="#reducing-the-dimensionality-of-data-with-neural-networks">Reducing the Dimensionality of Data with Neural Networks</a></li>
        <li><a href="#deep-fragment-embeddings-for-bidirectional-image-sentence-mapping">Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</a></li>
        <li><a href="#batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
        <li><a href="#bessel-s-correction">Bessel&rsquo;s Correction</a></li>
      </ul>
    </li>
    <li><a href="#building-makemore-part-5-building-a-wavenet">Building makemore Part 5: Building a WaveNet</a>
      <ul>
        <li><a href="#wavenet-a-generative-model-for-raw-audio">WaveNet: A Generative Model for Raw Audio</a></li>
        <li><a href="#bengio-et-al-2003-a-neural-probabilistic-language-model">bengio et al 2003 - A Neural Probabilistic Language Model</a></li>
      </ul>
    </li>
    <li><a href="#let-s-build-gpt-from-scratch-in-code-spelled-out-dot">Let&rsquo;s build GPT: from scratch, in code, spelled out.</a>
      <ul>
        <li><a href="#attention-is-all-you-need">Attention is All You Need</a></li>
        <li><a href="#layer-normalization">Layer Normalization</a></li>
        <li><a href="#dropout-a-simple-way-to-prevent-neural-networks-from-overfitting">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
        <li><a href="#language-models-are-few-shot-learners">Language Models are Few-Shot Learners</a></li>
        <li><a href="#introducing-chatgpt">Introducing ChatGPT</a></li>
      </ul>
    </li>
    <li><a href="#1hr-talk-intro-to-large-language-models">[1hr Talk] Intro to Large Language Models</a>
      <ul>
        <li><a href="#llama-2-open-foundation-and-fine-tuned-chat-models">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li>
        <li><a href="#training-language-models-to-follow-instructions-with-human-feedback">Training language models to follow instructions with human feedback</a></li>
        <li><a href="#tree-of-thoughts-deliberate-problem-solving-with-large-language-models-yao-et-al-dot-2023">Tree of Thoughts: Deliberate Problem Solving with Large Language Models, Yao et al. 2023</a></li>
        <li><a href="#training-compute-optimal-large-language-models">Training Compute-Optimal Large Language Models</a></li>
        <li><a href="#sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4-bubuck-et-al-dot-2023">Sparks of Artificial General Intelligence: Early experiments with GPT-4, Bubuck et al. 2023</a></li>
        <li><a href="#mastering-the-game-of-go-with-deep-neural-networks-and-tree-search">Mastering the game of Go with deep neural networks and tree search</a></li>
        <li><a href="#jailbroken-how-does-llm-safety-training-fail">Jailbroken: How Does LLM Safety Training Fail?</a></li>
        <li><a href="#universal-and-transferable-adversarial-attacks-on-aligned-language-models">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
        <li><a href="#visual-adversarial-examples-jailbreak-aligned-large-language-models">Visual Adversarial Examples Jailbreak Aligned Large Language Models</a></li>
        <li><a href="#not-what-you-ve-signed-up-for-compromising-real-world-llm-integrated-applications-with-indirect-prompt-injection">Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</a></li>
        <li><a href="#hacking-google-bard-from-prompt-injection-to-data-exfiltration">Hacking Google Bard - From Prompt Injection to Data Exfiltration</a></li>
        <li><a href="#poisoning-language-models-during-instruction-tuning">Poisoning Language Models During Instruction Tuning</a></li>
        <li><a href="#poisoning-web-scale-training-datasets-is-practical">Poisoning Web-Scale Training Datasets is Practical</a></li>
        <li><a href="#owasp-top-10-for-llm-applications">OWASP Top 10 for LLM Applications</a></li>
      </ul>
    </li>
    <li><a href="#let-s-build-the-gpt-tokenizer">Let&rsquo;s build the GPT Tokenizer</a>
      <ul>
        <li><a href="#language-models-are-unsupervised-multitask-learners">Language Models are Unsupervised Multitask Learners</a></li>
        <li><a href="#llama-2-open-foundation-and-fine-tuned-chat-models">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li>
        <li><a href="#a-programmer-s-introduction-to-unicode">A Programmer’s Introduction to Unicode</a></li>
        <li><a href="#megabyte-predicting-million-byte-sequences-with-multiscale-transformers">MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers</a></li>
        <li><a href="#efficient-training-of-language-models-to-fill-in-the-middle">Efficient Training of Language Models to Fill in the Middle</a></li>
        <li><a href="#learning-to-compress-prompts-with-gist-tokens">Learning to Compress Prompts with Gist Tokens</a></li>
        <li><a href="#taming-transformers-for-high-resolution-image-synthesis">Taming Transformers for High-Resolution Image Synthesis</a></li>
        <li><a href="#video-generation-models-as-world-simulators">Video generation models as world simulators</a></li>
        <li><a href="#integer-tokenization-is-insane">Integer tokenization is insane</a></li>
        <li><a href="#solidgoldmagikarp--plus-prompt-generation">SolidGoldMagikarp (plus, prompt generation)</a></li>
      </ul>
    </li>
    <li><a href="#let-s-reproduce-gpt-2--124m">Let&rsquo;s reproduce GPT-2 (124M)</a>
      <ul>
        <li><a href="#language-models-are-unsupervised-multitask-learners">Language Models are Unsupervised Multitask Learners</a></li>
        <li><a href="#language-models-are-few-shot-learners">Language Models are Few-Shot Learners</a></li>
        <li><a href="#attention-is-all-you-need">Attention is All You Need</a></li>
        <li><a href="#gaussian-error-linear-units--gelus">Gaussian Error Linear Units (GELUs)</a></li>
        <li><a href="#using-the-output-embedding-to-improve-language-models">Using the Output Embedding to Improve Language Models</a></li>
        <li><a href="#nvidia-a100-tensor-core-gpu-architecture">NVIDIA A100 Tensor Core GPU Architecture</a></li>
        <li><a href="#flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li>
        <li><a href="#flashattention-2-faster-attention-with-better-parallelism-and-work-partitioning">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></li>
        <li><a href="#online-normalizer-calculation-for-softmax">Online normalizer calculation for softmax</a></li>
        <li><a href="#hellaswag-can-a-machine-really-finish-your-sentence">HellaSwag: Can a Machine Really Finish Your Sentence?</a></li>
      </ul>
    </li>
    <li><a href="#deep-dive-into-llms-like-chatgpt">Deep Dive into LLMs like ChatGPT</a>
      <ul>
        <li><a href="#language-models-are-unsupervised-multitask-learners">Language Models are Unsupervised Multitask Learners</a></li>
        <li><a href="#the-llama-3-herd-of-models">The Llama 3 Herd of Models</a></li>
        <li><a href="#training-language-models-to-follow-instructions-with-human-feedback">Training language models to follow instructions with human feedback</a></li>
        <li><a href="#deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></li>
        <li><a href="#mastering-the-game-of-go-without-human-knowledge">Mastering the Game of Go without Human Knowledge</a></li>
        <li><a href="#fine-tuning-language-models-from-human-preferences">Fine-Tuning Language Models from Human Preferences</a></li>
      </ul>
    </li>
    <li><a href="#how-i-use-llms">How I use LLMs</a>
      <ul>
        <li><a href="#deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
    

    <div id="share-footer" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f" aria-label="Facebook">
      <i class="fab fa-facebook fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&text=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="Twitter">
      <i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&title=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="Linkedin">
      <i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&is_video=false&description=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="Pinterest">
      <i class="fab fa-pinterest fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos&body=Check out this article: https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f" aria-label="Email">
      <i class="fas fa-envelope fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&title=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="Pocket">
      <i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&title=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="reddit">
      <i class="fab fa-reddit fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&name=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos&description=%3cp%3eI%26rsquo%3bve%20been%20following%20%3ca%20href%3d%22https%3a%2f%2fwww.youtube.com%2f%40AndrejKarpathy%2fvideos%22%3eAndrej%20Karpathy%26rsquo%3bs%20YouTube%20channel%3c%2fa%3e%20for%20a%20long%20time%2c%20and%20he%0amade%20me%20want%20to%20dive%20deep%20into%20AI%2c%20Neural%20Networks%20and%20LLMs.%20His%20videos%20are%0ainteresting%2c%20precise%20and%20engaging%2c%20and%20they%20pushed%20me%20to%20read%20primary%20sources%2c%0ai.e.%20papers%20and%20articles.%20I%26rsquo%3bve%20collected%20here%20all%20the%20papers%20that%20were%20mentioned%0ain%20the%20various%20videos%2c%20as%20a%20personal%20reminder%20to%20read%20them%20all.%3c%2fp%3e%0a%3cp%3eSince%20I%26rsquo%3bve%20already%20seen%20all%20the%20videos%20and%20didn%26rsquo%3bt%20want%20to%20watch%20them%20all%20again%0ajust%20to%20find%20the%20papers%2c%20I%26rsquo%3bve%20downloaded%20the%20subtitle%20with%20the%20command%20below%2c%0aand%20searched%20for%20all%20the%20occurrences%20of%20the%20term%20%26ldquo%3bpaper%26rdquo%3b%2c%20then%20looked%20at%20the%0acorresponding%20timestamps.%3c%2fp%3e" aria-label="Tumblr">
      <i class="fab fa-tumblr fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fgiampaolo.guiducci.it%2fposts%2f2026-02-09-karpathy-video-papers%2f&t=All%20the%20papers%20cited%20in%20Andrej%20Karpathy%27s%20YouTube%20videos" aria-label="Hacker News">
      <i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>

    <div id="actions-footer">
      
        <a id="menu-toggle" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;" aria-label="Menu">
          <i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
        <a id="toc-toggle" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;" aria-label="TOC">
          <i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share-toggle" class="icon" href="#" onclick="$('#share-footer').toggle();return false;" aria-label="Share">
          <i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
          <i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>


  <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2026  Giampaolo Guiducci 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">home</a></li>
         
        <li><a href="/posts">posts</a></li>
         
        <li><a href="/%CE%BClog">μlog</a></li>
         
        <li><a href="/tags">tags</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>

<script src=/js/code-copy.js></script>




</html>
